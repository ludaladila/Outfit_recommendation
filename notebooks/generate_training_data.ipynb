{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec094d0",
      "metadata": {
        "id": "0ec094d0"
      },
      "outputs": [],
      "source": [
        "# Clean environment: downgrade numpy & reinstall transformers\n",
        "!pip uninstall -y numpy transformers\n",
        "!pip install numpy==1.24.4 --no-cache-dir --force-reinstall\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install accelerate bitsandbytes xformers pandas openpyxl --upgrade\n",
        "\n",
        "# Restart runtime to fully apply changes\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install HuggingFace Transformers from the latest GitHub repo\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n"
      ],
      "metadata": {
        "id": "alcwQU_XHxzH"
      },
      "id": "alcwQU_XHxzH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jax --upgrade"
      ],
      "metadata": {
        "id": "7h4BfRHZISa6"
      },
      "id": "7h4BfRHZISa6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-PSU9-OqNBzF",
      "metadata": {
        "id": "-PSU9-OqNBzF"
      },
      "outputs": [],
      "source": [
        "# ✅ Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, AutoTokenizer\n",
        "\n",
        "# Load Model & Processor\n",
        "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    \"llava-hf/llava-1.5-7b-hf\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# File Paths\n",
        "image_base_path = \"/content/drive/MyDrive/fashion_dataset/data/\"\n",
        "cluster_csv = \"/content/drive/MyDrive/fashion_dataset/image_clusters.csv\"\n",
        "excel_path = \"/content/drive/MyDrive/fashion_dataset/data.xlsx\"\n",
        "\n",
        "# Load Data\n",
        "def load_cluster_data(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df[\"filename\"] = df[\"image_path\"].apply(lambda x: os.path.basename(x))\n",
        "    return df\n",
        "\n",
        "def load_excel_descriptions(excel_path):\n",
        "    df = pd.read_excel(excel_path)\n",
        "    df[\"filename\"] = df[\"main_image_url\"].apply(lambda x: os.path.basename(x))\n",
        "    return dict(zip(df[\"filename\"], df[\"description\"]))\n",
        "\n",
        "def build_outfit(anchor_row, df_all):\n",
        "    anchor_cluster = anchor_row[\"cluster\"]\n",
        "    anchor_img = anchor_row[\"image_path\"]\n",
        "    outfit = {\"input_image\": anchor_img, f\"cluster_{anchor_cluster}\": anchor_img}\n",
        "    for c in df_all[\"cluster\"].unique():\n",
        "        if c == anchor_cluster:\n",
        "            continue\n",
        "        candidates = df_all[df_all[\"cluster\"] == c]\n",
        "        selected = candidates.sample(1).iloc[0]\n",
        "        outfit[f\"cluster_{c}\"] = selected[\"image_path\"]\n",
        "    return outfit\n",
        "\n",
        "# Format Prompt\n",
        "def format_outfit_prompt(outfit, descriptions):\n",
        "    anchor_img = os.path.basename(outfit[\"input_image\"])\n",
        "    desc = descriptions.get(anchor_img, \"\")\n",
        "    return f\"\"\"The main item description is: {desc}\n",
        "\n",
        "You are a fashion evaluator. First, look at the image and **list all the clothing items you see**, specifying their types (e.g., '1 t-shirt, 1 pair of jeans, 1 hoodie').\n",
        "\n",
        "Then determine whether the outfit contains at least one **top** and one **bottom**.\n",
        "\n",
        "Rules:\n",
        "- A valid outfit must include **at least one top and one bottom**.\n",
        "- If the outfit includes **only tops** or **only bottoms**, give it a **low score (1–3)**.\n",
        "- If the outfit includes both a top and a bottom and they match well in style and color, give a **higher score (8–10)**.\n",
        "- Use the full score range from 1 to 10.\n",
        "\n",
        "In your response, do the following:\n",
        "1. List the items you see(e.g., top, bottom, others).\n",
        "2. Explain your reasoning in 1–2 sentences.\n",
        "3. On a new line, write only: Score: X (e.g., Score: 7)\n",
        "\n",
        "ASSISTANT:\"\"\"\n",
        "\n",
        "# Extract Score\n",
        "def extract_score(result):\n",
        "    lines = result.splitlines()\n",
        "    assistant_indices = [i for i, line in enumerate(lines) if line.strip().upper() == \"ASSISTANT:\"]\n",
        "    if assistant_indices:\n",
        "        last_idx = assistant_indices[-1]\n",
        "        lines = lines[last_idx + 1:]\n",
        "    for line in lines:\n",
        "        if \"score\" in line.lower():\n",
        "            match = re.search(r\"Score[:：]?\\s*(\\d+)\", line, re.IGNORECASE)\n",
        "            if match:\n",
        "                return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "# Collage Creator\n",
        "def create_outfit_collage(outfit, image_base_path, size=(256, 256)):\n",
        "    images = []\n",
        "    for k in outfit:\n",
        "        if k == \"input_image\" or k.startswith(\"cluster_\"):\n",
        "            filename = os.path.basename(outfit[k]).replace(\"\\\\\", \"/\").split(\"/\")[-1]\n",
        "            path = os.path.join(image_base_path, filename)\n",
        "            try:\n",
        "                img = Image.open(path).convert(\"RGB\").resize(size)\n",
        "                images.append(img)\n",
        "            except:\n",
        "                continue\n",
        "    if not images:\n",
        "        return None\n",
        "    total_width = size[0] * len(images)\n",
        "    collage = Image.new(\"RGB\", (total_width, size[1]))\n",
        "    for i, img in enumerate(images):\n",
        "        collage.paste(img, (i * size[0], 0))\n",
        "    return collage\n",
        "\n",
        "# LLaVA Scoring\n",
        "def get_llava_score_with_huggingface(image, prompt_text):\n",
        "    try:\n",
        "        prompt = f\"<image>\\nUSER: {prompt_text.strip()}\\nASSISTANT:\"\n",
        "        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "        output = model.generate(**inputs, max_new_tokens=300)\n",
        "        result = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "        score = extract_score(result)\n",
        "        if score is not None:\n",
        "            return min(max(score, 1), 10), result\n",
        "        else:\n",
        "            return np.random.randint(5, 8), result\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return np.random.randint(5, 8), \"Error\"\n",
        "\n",
        "# Run Scoring\n",
        "df = load_cluster_data(cluster_csv)\n",
        "desc_map = load_excel_descriptions(excel_path)\n",
        "df_anchors = df[df[\"filename\"].str.endswith(\"_1.jpg\")].sample(n=700, random_state=42).copy()\n",
        "\n",
        "outfits = []\n",
        "scores = []\n",
        "\n",
        "for i, (_, row) in enumerate(df_anchors.iterrows(), 1):\n",
        "    outfit = build_outfit(row, df)\n",
        "    prompt = format_outfit_prompt(outfit, desc_map)\n",
        "    collage = create_outfit_collage(outfit, image_base_path)\n",
        "\n",
        "    if collage is None:\n",
        "        print(f\"{i}: Could not create collage. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing {i}/{len(df_anchors)}: {os.path.basename(outfit['input_image'])}\")\n",
        "    score, response = get_llava_score_with_huggingface(collage, prompt)\n",
        "    outfit[\"llm_score\"] = score\n",
        "    outfit[\"llm_response\"] = response\n",
        "    outfits.append(outfit)\n",
        "    scores.append(score)\n",
        "    print(f\"Score: {score}\")\n",
        "\n",
        "# Save Results\n",
        "avg_score = np.mean(scores)\n",
        "print(f\"\\nAverage LLaVA Score: {avg_score:.2f}\")\n",
        "pd.DataFrame(outfits).to_csv(\"llava_outfit_scores.csv\", index=False)\n",
        "print(\"Saved to: llava_outfit_scores.csv\")\n"
      ],
      "metadata": {
        "id": "flkPhSYqxrVW"
      },
      "id": "flkPhSYqxrVW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}