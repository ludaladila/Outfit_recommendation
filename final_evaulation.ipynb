{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N64YaCm0nZL-",
      "metadata": {
        "id": "N64YaCm0nZL-"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y numpy transformers\n",
        "!pip install numpy==1.24.4 --no-cache-dir --force-reinstall\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install accelerate bitsandbytes xformers pandas openpyxl --upgrade\n",
        "!pip install scikit-learn pandas\n",
        "\n",
        "\n",
        "# Restart runtime to fully apply changes\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V_4pbGTPpN1q",
      "metadata": {
        "id": "V_4pbGTPpN1q"
      },
      "outputs": [],
      "source": [
        "# Install HuggingFace Transformers from the latest GitHub repo\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HMUbhinOnh2Z",
      "metadata": {
        "id": "HMUbhinOnh2Z"
      },
      "outputs": [],
      "source": [
        "!pip install jax --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vNEsI2WjnZL9",
      "metadata": {
        "id": "vNEsI2WjnZL9"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M7JT6yXsnZL_",
      "metadata": {
        "id": "M7JT6yXsnZL_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import product\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Paths\n",
        "base_path = \"/content/drive/MyDrive/fashion_dataset\"\n",
        "image_base_path = f\"{base_path}/data/\"\n",
        "cluster_csv = f\"{base_path}/image_clusters.csv\"\n",
        "excel_path = f\"{base_path}/data.xlsx\"\n",
        "test_data = f\"{base_path}/test_anchor_images.csv\"\n",
        "model_path = f\"{base_path}/rf_regressor_model.pkl\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ILaqyQRznZL_",
      "metadata": {
        "id": "ILaqyQRznZL_"
      },
      "outputs": [],
      "source": [
        "# Load LLaVA\n",
        "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
        "llava_model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    \"llava-hf/llava-1.5-7b-hf\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wy4NDjDIyAnD",
      "metadata": {
        "id": "Wy4NDjDIyAnD"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv(cluster_csv)\n",
        "df[\"filename\"] = df[\"image_path\"].apply(lambda x: os.path.basename(str(x).strip().replace(\"\\\\\", \"/\")))\n",
        "df_embed = df.set_index(\"filename\")[[str(i) for i in range(512)]]\n",
        "df_desc = pd.read_excel(excel_path)\n",
        "desc_map = dict(zip(df_desc[\"main_image_url\"].apply(lambda x: os.path.basename(str(x))), df_desc[\"description\"]))\n",
        "test_anchors = pd.read_csv(test_data)[\"test_anchors\"].apply(lambda x: os.path.basename(str(x).strip().replace(\"\\\\\", \"/\"))).tolist()\n",
        "model = joblib.load(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yZYFtGeanZMA",
      "metadata": {
        "id": "yZYFtGeanZMA"
      },
      "outputs": [],
      "source": [
        "def get_llava_score(image_path, description):\n",
        "    try:\n",
        "        prompt = (\n",
        "          \"<image>\\n\"\n",
        "          f\"USER: The main item description is: {description}.\\n\\n\"\n",
        "          \"You are a fashion evaluator. First, look at the image and **list all the clothing items you see**, specifying their types (e.g., '1 t-shirt, 1 pair of jeans, 1 hoodie').\\n\\n\"\n",
        "          \"Then determine whether the outfit contains at least one **top** and one **bottom**.\\n\\n\"\n",
        "          \"Rules:\\n\"\n",
        "          \"- A valid outfit must include **at least one top and one bottom**.\\n\"\n",
        "          \"- If the outfit includes **only tops** or **only bottoms**, give it a **low score (1–3)**.\\n\"\n",
        "          \"- If the outfit includes both a top and a bottom and they match well in style and color, give a **higher score (8–10)**.\\n\"\n",
        "          \"- Use the full score range from 1 to 10.\\n\\n\"\n",
        "          \"In your response, do the following:\\n\"\n",
        "          \"1. List the items you see(e.g., top, bottom, others).\\n\"\n",
        "          \"2. Explain your reasoning in 1–2 sentences.\\n\"\n",
        "          \"3. On a new line, write only: Score: X (e.g., Score: 7)\\n\\n\"\n",
        "          \"ASSISTANT:\"\n",
        "      )\n",
        "\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
        "\n",
        "        inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(\"cuda\", torch.float16)\n",
        "        inputs[\"input_ids\"] = inputs[\"input_ids\"].to(\"cuda\")\n",
        "        inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(\"cuda\")\n",
        "\n",
        "        output = llava_model.generate(**inputs, max_new_tokens=300)\n",
        "        result = processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "        import re\n",
        "        matches = re.findall(r\"(?:score\\s*(?:of|is|:)?|rated)\\s*(\\d+)\", result, re.IGNORECASE)\n",
        "        if matches:\n",
        "            score = int(matches[-1])  # use the last one\n",
        "            return min(max(score, 1), 10)\n",
        "        else:\n",
        "            print(f\"Couldn't extract score from response: {result}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with LLaVA for {image_path}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VN7DXBsKnZMA",
      "metadata": {
        "id": "VN7DXBsKnZMA"
      },
      "outputs": [],
      "source": [
        "# Build recommendations\n",
        "cluster_dict = df.groupby(\"cluster\")[\"filename\"].apply(list).to_dict()\n",
        "image_path_dict = dict(zip(df[\"filename\"], df[\"image_path\"]))\n",
        "\n",
        "TOP_K = 3\n",
        "recommendations = []\n",
        "\n",
        "for anchor in test_anchors:\n",
        "    print(f\"Processing anchor: {anchor}\")\n",
        "    if anchor not in df_embed.index:\n",
        "        print(f\"Missing embedding for anchor: {anchor}\")\n",
        "        continue\n",
        "\n",
        "    anchor_row = df[df[\"filename\"] == anchor].iloc[0]\n",
        "    anchor_cluster = anchor_row[\"cluster\"]\n",
        "    anchor_vec = df_embed.loc[anchor].values.reshape(1, -1)\n",
        "\n",
        "    top_k_per_cluster = {}\n",
        "    for c in cluster_dict:\n",
        "        if c == anchor_cluster:\n",
        "            continue\n",
        "        candidates = cluster_dict[c]\n",
        "        valid = [f for f in candidates if f in df_embed.index]\n",
        "        if not valid:\n",
        "            continue\n",
        "        vecs = np.stack([df_embed.loc[f] for f in valid])\n",
        "        sims = cosine_similarity(anchor_vec, vecs)[0]\n",
        "        idx = np.argsort(sims)[::-1][:TOP_K]\n",
        "        top_k_per_cluster[c] = [valid[i] for i in idx]\n",
        "\n",
        "    if len(top_k_per_cluster) < 2:\n",
        "        print(f\"Skipping {anchor} — not enough clusters.\")\n",
        "        continue\n",
        "\n",
        "    combos = list(product(*top_k_per_cluster.values()))\n",
        "    best_score = -1\n",
        "    best_outfit = None\n",
        "\n",
        "    for combo in combos:\n",
        "        try:\n",
        "            vecs = [df_embed.loc[anchor]] + [df_embed.loc[x] for x in combo]\n",
        "            full_vec = np.concatenate(vecs).reshape(1, -1)\n",
        "            score = model.predict(full_vec)[0]\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_outfit = [anchor] + list(combo)\n",
        "        except Exception as e:\n",
        "            print(f\"Error scoring combo: {e}\")\n",
        "            continue\n",
        "\n",
        "    if best_outfit:\n",
        "        # get description for anchor\n",
        "        anchor_file = os.path.basename(image_path_dict[best_outfit[0]].replace(\"\\\\\", \"/\"))\n",
        "        description = desc_map.get(anchor_file, \"\")\n",
        "\n",
        "        print(f\"Looking for description with key: {anchor_file}\")\n",
        "        print(f\"Description: {description}\")\n",
        "\n",
        "        if not description or not isinstance(description, str) or description.strip() == \"\":\n",
        "            print(f\"No description for {anchor_file}, skipping...\")\n",
        "            continue\n",
        "\n",
        "        from PIL import Image\n",
        "\n",
        "        # fix path formatting for Colab\n",
        "        relative = image_path_dict[best_outfit[0]].replace(\"\\\\\", \"/\")\n",
        "        image_path = os.path.join(base_path, relative)\n",
        "\n",
        "        try:\n",
        "            img = Image.open(image_path)\n",
        "            img.verify()\n",
        "            print(f\"Image at {image_path} is valid.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Image open failed: {image_path} | Error: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        description = desc_map.get(anchor_file, \"\")\n",
        "\n",
        "        # score with LLaVA\n",
        "\n",
        "        llava_val = get_llava_score(image_path, description)\n",
        "        # Show final score you're saving\n",
        "        print(f\"Final score to save for {anchor}: {llava_val}\")\n",
        "        recommendations.append({\n",
        "            \"anchor\": image_path_dict[best_outfit[0]],\n",
        "            \"match_1\": image_path_dict[best_outfit[1]],\n",
        "            \"match_2\": image_path_dict[best_outfit[2]],\n",
        "            \"model_score\": round(best_score * 10, 2),\n",
        "            \"llava_score\": llava_val\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jIbAggv9ziKO",
      "metadata": {
        "id": "jIbAggv9ziKO"
      },
      "outputs": [],
      "source": [
        "# Save results\n",
        "df_result = pd.DataFrame(recommendations)\n",
        "df_result.to_csv(\"llava_similarity_model_selected.csv\", index=False)\n",
        "print(\"Saved to llava_similarity_model_selected.csv\")\n",
        "if len(df_result):\n",
        "    print(f\"Average LLaVA Score: {np.mean([r for r in df_result['llava_score'] if pd.notnull(r)]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a224c50",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Replace these with the actual score column names if different\n",
        "input_scores = df_result [\"model_score\"]\n",
        "anchor_scores = df_result [\"llava_score\"]\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(input_scores, anchor_scores)\n",
        "print(f\"Mean Squared Error (MSE) between ml model and llava scores: {mse:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
